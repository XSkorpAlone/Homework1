{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re\n",
        "\n",
        "# Global counter for generating fresh variable names for alpha conversion\n",
        "fresh_var_counter = 0\n",
        "def fresh_var(base):\n",
        "    global fresh_var_counter\n",
        "    fresh_var_counter += 1\n",
        "    return f\"{base}_{fresh_var_counter}\"\n",
        "\n",
        "# Abstract Syntax Tree (AST) Node Definitions\n",
        "\n",
        "class Term:\n",
        "    def free_vars(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def substitute(self, var, value):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __str__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Variable(Term):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def free_vars(self):\n",
        "        return {self.name}\n",
        "\n",
        "    def substitute(self, var, value):\n",
        "        if self.name == var:\n",
        "            return value\n",
        "        else:\n",
        "            return self\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.name\n",
        "\n",
        "class Abstraction(Term):\n",
        "    def __init__(self, param, body):\n",
        "        self.param = param\n",
        "        self.body = body\n",
        "\n",
        "    def free_vars(self):\n",
        "        # Free variables of the abstraction are those free in the body, minus the bound parameter.\n",
        "        return self.body.free_vars() - {self.param}\n",
        "\n",
        "    def substitute(self, var, value):\n",
        "        if self.param == var:\n",
        "            return self\n",
        "        elif self.param in value.free_vars():\n",
        "            # To avoid variable capture, perform alpha conversion.\n",
        "            new_param = fresh_var(self.param)\n",
        "            new_body = self.body.substitute(self.param, Variable(new_param))\n",
        "            return Abstraction(new_param, new_body.substitute(var, value))\n",
        "        else:\n",
        "            return Abstraction(self.param, self.body.substitute(var, value))\n",
        "\n",
        "    def __str__(self):\n",
        "        # Using the λ notation for abstraction.\n",
        "        return f\"(λ{self.param}.{self.body})\"\n",
        "\n",
        "class Application(Term):\n",
        "    def __init__(self, func, arg):\n",
        "        self.func = func\n",
        "        self.arg = arg\n",
        "\n",
        "    def free_vars(self):\n",
        "        # Free variables are the union of free variables in both the function and the argument.\n",
        "        return self.func.free_vars() | self.arg.free_vars()\n",
        "\n",
        "    def substitute(self, var, value):\n",
        "        return Application(self.func.substitute(var, value),\n",
        "                           self.arg.substitute(var, value))\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"({self.func} {self.arg})\"\n",
        "\n",
        "# Lexer and Parser for Lambda Expressions\n",
        "\n",
        "# Regular expression to tokenize input. This pattern matches:\n",
        "#  - Left parentheses: \"(\"\n",
        "#  - Right parentheses: \")\"\n",
        "#  - Dot: \".\"\n",
        "#  - Lambda symbol: \"\\\" or \"λ\"\n",
        "#  - Identifiers: sequences of letters, digits, or underscores starting with a letter or underscore.\n",
        "token_pattern = re.compile(r'\\s*(?:(\\()|(\\))|(\\.)|(\\\\|λ)|([A-Za-z_][A-Za-z0-9_]*))')\n",
        "\n",
        "def tokenize(s):\n",
        "    tokens = []\n",
        "    pos = 0\n",
        "    while pos < len(s):\n",
        "        match = token_pattern.match(s, pos)\n",
        "        if not match:\n",
        "            raise SyntaxError(f\"Unexpected character at position {pos}: {s[pos]}\")\n",
        "        if match.group(1):\n",
        "            tokens.append(('LPAREN', '('))\n",
        "        elif match.group(2):\n",
        "            tokens.append(('RPAREN', ')'))\n",
        "        elif match.group(3):\n",
        "            tokens.append(('DOT', '.'))\n",
        "        elif match.group(4):\n",
        "            tokens.append(('LAMBDA', 'λ'))\n",
        "        elif match.group(5):\n",
        "            tokens.append(('IDENT', match.group(5)))\n",
        "        pos = match.end()\n",
        "    return tokens\n",
        "\n",
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.pos = 0\n",
        "\n",
        "    def peek(self):\n",
        "        if self.pos < len(self.tokens):\n",
        "            return self.tokens[self.pos]\n",
        "        return None\n",
        "\n",
        "    def consume(self, expected_type=None):\n",
        "        token = self.peek()\n",
        "        if token is None:\n",
        "            return None\n",
        "        if expected_type and token[0] != expected_type:\n",
        "            raise SyntaxError(f\"Expected token type {expected_type}, got {token}\")\n",
        "        self.pos += 1\n",
        "        return token\n",
        "\n",
        "    def parse(self):\n",
        "        term = self.parse_expr()\n",
        "        if self.pos != len(self.tokens):\n",
        "            raise SyntaxError(\"Extra tokens at end of input\")\n",
        "        return term\n",
        "\n",
        "    def parse_expr(self):\n",
        "        # An expression may be an abstraction or an application.\n",
        "        token = self.peek()\n",
        "        if token and token[0] == 'LAMBDA':\n",
        "            return self.parse_abstraction()\n",
        "        else:\n",
        "            return self.parse_application()\n",
        "\n",
        "    def parse_abstraction(self):\n",
        "        # Abstraction syntax: LAMBDA IDENT+ DOT expr\n",
        "        self.consume('LAMBDA')\n",
        "        params = []\n",
        "        while True:\n",
        "            token = self.peek()\n",
        "            if token is None or token[0] != 'IDENT':\n",
        "                break\n",
        "            params.append(self.consume('IDENT')[1])\n",
        "        if not params:\n",
        "            raise SyntaxError(\"Expected at least one parameter in abstraction\")\n",
        "        if self.peek() is None or self.peek()[0] != 'DOT':\n",
        "            raise SyntaxError(\"Expected '.' after parameters in abstraction\")\n",
        "        self.consume('DOT')\n",
        "        body = self.parse_expr()\n",
        "        # Convert multi-parameter abstractions into nested abstractions.\n",
        "        for param in reversed(params):\n",
        "            body = Abstraction(param, body)\n",
        "        return body\n",
        "\n",
        "    def parse_application(self):\n",
        "        # Parse one or more atoms and treat them as left-associative applications.\n",
        "        term = self.parse_atom()\n",
        "        while True:\n",
        "            token = self.peek()\n",
        "            if token is None or token[0] in ('RPAREN', 'DOT'):\n",
        "                break\n",
        "            next_atom = self.parse_atom()\n",
        "            term = Application(term, next_atom)\n",
        "        return term\n",
        "\n",
        "    def parse_atom(self):\n",
        "        token = self.peek()\n",
        "        if token is None:\n",
        "            raise SyntaxError(\"Unexpected end of input\")\n",
        "        if token[0] == 'IDENT':\n",
        "            return Variable(self.consume('IDENT')[1])\n",
        "        elif token[0] == 'LPAREN':\n",
        "            self.consume('LPAREN')\n",
        "            term = self.parse_expr()\n",
        "            if self.peek() is None or self.peek()[0] != 'RPAREN':\n",
        "                raise SyntaxError(\"Expected ')'\")\n",
        "            self.consume('RPAREN')\n",
        "            return term\n",
        "        elif token[0] == 'LAMBDA':\n",
        "            # Allow an abstraction to appear where an atom is expected.\n",
        "            return self.parse_abstraction()\n",
        "        else:\n",
        "            raise SyntaxError(f\"Unexpected token: {token}\")\n",
        "\n",
        "def parse_lambda(s):\n",
        "    tokens = tokenize(s)\n",
        "    parser = Parser(tokens)\n",
        "    return parser.parse()\n",
        "\n",
        "# Reduction Functions\n",
        "\n",
        "def reduce_term(term, max_steps=1000):\n",
        "    \"\"\"Reduce a lambda term using beta reduction until no more reductions can be made\n",
        "       or until a step limit is reached (to avoid non-termination).\"\"\"\n",
        "    current = term\n",
        "    for _ in range(max_steps):\n",
        "        new_term, reduced = reduce_once(current)\n",
        "        if not reduced:\n",
        "            return new_term\n",
        "        current = new_term\n",
        "    print(\"Reduction did not terminate within step limit.\")\n",
        "    return current\n",
        "\n",
        "def reduce_once(term):\n",
        "    \"\"\"Perform one reduction step on the term. Returns a tuple (new_term, reduction_occurred).\"\"\"\n",
        "    if isinstance(term, Application):\n",
        "        if isinstance(term.func, Abstraction):\n",
        "            # Beta reduction: ((λx.M) N) → M[x := N]\n",
        "            return (term.func.body.substitute(term.func.param, term.arg), True)\n",
        "        else:\n",
        "            # Try reducing the function part.\n",
        "            new_func, reduced = reduce_once(term.func)\n",
        "            if reduced:\n",
        "                return (Application(new_func, term.arg), True)\n",
        "            else:\n",
        "                # Try reducing the argument.\n",
        "                new_arg, reduced = reduce_once(term.arg)\n",
        "                if reduced:\n",
        "                    return (Application(term.func, new_arg), True)\n",
        "                else:\n",
        "                    return (term, False)\n",
        "    elif isinstance(term, Abstraction):\n",
        "        new_body, reduced = reduce_once(term.body)\n",
        "        if reduced:\n",
        "            return (Abstraction(term.param, new_body), True)\n",
        "        else:\n",
        "            return (term, False)\n",
        "    else:\n",
        "        # No reduction possible.\n",
        "        return (term, False)\n",
        "\n",
        "\n",
        "def perform():\n",
        "    print(\"Lambda Calculus Interpreter\")\n",
        "    print(\"Enter lambda expressions using '\\\\' or 'λ' for abstraction.\")\n",
        "    print(\"For example: (\\\\x. x) y\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    while True:\n",
        "        try:\n",
        "            s = input(\"λ> \")\n",
        "        except EOFError:\n",
        "            break\n",
        "        if s.strip() == \"quit\":\n",
        "            break\n",
        "        if s.strip() == \"\":\n",
        "            continue\n",
        "        try:\n",
        "            term = parse_lambda(s)\n",
        "            print(\"Parsed term:\", term)\n",
        "            reduced = reduce_term(term)\n",
        "            print(\"Reduced term:\", reduced)\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "6kx8dDRIVl03"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if len(sys.argv) > 1 and os.path.isfile(sys.argv[1]):\n",
        "    with open(sys.argv[1], 'r') as f:\n",
        "        s = f.read()\n",
        "    try:\n",
        "        term = parse_lambda(s)\n",
        "        print(\"Parsed term:\", term)\n",
        "        reduced = reduce_term(term)\n",
        "        print(\"Reduced term:\", reduced)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "else:\n",
        "    perform()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RiJbQg0Zc0q",
        "outputId": "83d9329e-810e-4fba-8019-521499e68ee3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lambda Calculus Interpreter\n",
            "Enter lambda expressions using '\\' or 'λ' for abstraction.\n",
            "For example: (\\x. x) y\n",
            "Type 'quit' to exit.\n",
            "λ> λx.x\n",
            "Parsed term: (λx.x)\n",
            "Reduced term: (λx.x)\n",
            "λ> (λx.x) y\n",
            "Parsed term: ((λx.x) y)\n",
            "Reduced term: y\n",
            "λ> (λx. x x) (λy. y)\n",
            "Parsed term: ((λx.(x x)) (λy.y))\n",
            "Reduced term: (λy.y)\n",
            "λ> (λx. x) ((λy. y) z)\n",
            "Parsed term: ((λx.x) ((λy.y) z))\n",
            "Reduced term: z\n",
            "λ> (λn. λf. λx. f (n f x)) λf. λx. f x\n",
            "Parsed term: ((λn.(λf.(λx.(f ((n f) x))))) (λf.(λx.(f x))))\n",
            "Reduced term: (λf.(λx.(f (f x))))\n",
            "λ> (λp. λq. p q p) (λx. λy. x) (λx. λy. y)\n",
            "Parsed term: (((λp.(λq.((p q) p))) (λx.(λy.x))) (λx.(λy.y)))\n",
            "Reduced term: (λx.(λy.y))\n",
            "λ> quit\n"
          ]
        }
      ]
    }
  ]
}